{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import logging\n",
    "import ssl\n",
    "\n",
    "# ---- HTML parsing ----\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---- PDF parsing ----\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import requests\n",
    "# Quiet down pdfminer warnings (e.g., gray stroke color issues)\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "# ---- NLP & Visualization ----\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as vis\n",
    "\n",
    "# Try to ensure NLTK resources; fall back silently if downloads fail (e.g., SSL issues)\n",
    "USE_NLTK = True\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "    except Exception:\n",
    "        USE_NLTK = False\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "    except Exception:\n",
    "        USE_NLTK = False\n",
    "if not USE_NLTK:\n",
    "    # Minimal built-in stopword set as a fallback\n",
    "    FALLBACK_STOPWORDS = {\n",
    "        \"the\",\"and\",\"to\",\"of\",\"in\",\"a\",\"is\",\"it\",\"that\",\"for\",\"on\",\"as\",\"with\",\"this\",\"by\",\"an\",\"are\",\"be\",\n",
    "        \"or\",\"from\",\"at\",\"was\",\"but\",\"not\",\"have\",\"has\",\"had\",\"were\",\"which\",\"their\",\"its\",\"they\",\"we\",\"you\",\n",
    "        \"your\",\"our\",\"can\",\"will\",\"would\",\"should\",\"could\",\"about\",\"into\",\"over\",\"than\",\"so\",\"no\",\"yes\",\"if\",\n",
    "        \"when\",\"while\",\"what\",\"who\",\"whom\",\"where\",\"why\",\"how\",\"all\",\"any\",\"each\",\"few\",\"more\",\"most\",\"other\",\n",
    "        \"some\",\"such\",\"only\",\"own\",\"same\",\"both\",\"very\",\"s\",\"t\",\"just\",\"don\",\"now\"\n",
    "    }\n",
    "    # Simple regex tokenizers as fallback (no NLTK corpora required)\n",
    "    import re as _re\n",
    "    def _simple_sent_tokenize(text: str):\n",
    "        return [s.strip() for s in _re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "    def _simple_word_tokenize(text: str):\n",
    "        return _re.findall(r\"[A-Za-z']+\", text)\n",
    "\n",
    "# ---------------------------- USER CONFIG ----------------------------\n",
    "ebook_url = \"https://clubphysical.co.nz/wp-content/uploads/2025/06/How-to-build-your-body-E-Book-updated.pdf.pdf\"\n",
    "MAKE_CLOUDS = True           # generate a wordcloud visualization\n",
    "CLOUD_INTERVAL = 10.0        # seconds between successive word clouds\n",
    "RUN_FREQ_VIS = True          # show frequency distribution plot\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "class EBOOK_web_scraper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cleaner_Data = \"\"\n",
    "        self.all_words = []\n",
    "        self.meaningful_words = []\n",
    "        self._last_cloud_time = 0.0\n",
    "        self._is_pdf = False\n",
    "        self._tmp_pdf_path = None\n",
    "        self._use_nltk = USE_NLTK\n",
    "        if self._use_nltk:\n",
    "            try:\n",
    "                self._stopwords = set(stopwords.words(\"english\"))\n",
    "            except Exception:\n",
    "                self._use_nltk = False\n",
    "                self._stopwords = FALLBACK_STOPWORDS\n",
    "        else:\n",
    "            self._stopwords = FALLBACK_STOPWORDS\n",
    "\n",
    "    def _is_pdf_url(self, url: str) -> bool:\n",
    "        path = urlparse(url).path.lower()\n",
    "        return path.endswith(\".pdf\")\n",
    "\n",
    "    def accessData(self, url: str):\n",
    "        \"\"\"\n",
    "        Load the ebook from URL.\n",
    "        - If PDF: extract text page-by-page and concatenate.\n",
    "        - If HTML: fetch, strip tags, and normalize whitespace.\n",
    "        Stores the cleaned text in self.cleaner_Data.\n",
    "        \"\"\"\n",
    "        self._is_pdf = self._is_pdf_url(url)\n",
    "\n",
    "        if self._is_pdf:\n",
    "            # ---- PDF path ----\n",
    "            content = requests.get(url, timeout=60).content\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as tmp:\n",
    "                tmp.write(content)\n",
    "                self._tmp_pdf_path = Path(tmp.name)\n",
    "\n",
    "            # Count pages (optional, not printed)\n",
    "            with open(self._tmp_pdf_path, \"rb\") as f:\n",
    "                _ = sum(1 for _ in PDFPage.get_pages(f))\n",
    "\n",
    "            # Extract text from all pages and join\n",
    "            all_pages = []\n",
    "            text_all = \"\"\n",
    "            try:\n",
    "                text_all = extract_text(str(self._tmp_pdf_path)) or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] pdfminer failed: {e}\")\n",
    "                text_all = \"\"\n",
    "            if not text_all.strip():\n",
    "                try:\n",
    "                    from pypdf import PdfReader\n",
    "                    reader = PdfReader(str(self._tmp_pdf_path))\n",
    "                    buf = []\n",
    "                    for page in reader.pages:\n",
    "                        try:\n",
    "                            buf.append(page.extract_text() or \"\")\n",
    "                        except Exception:\n",
    "                            buf.append(\"\")\n",
    "                    text_all = \" \".join(buf)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] pypdf fallback failed: {e}\")\n",
    "            all_pages.append(text_all)\n",
    "\n",
    "            joined = \" \".join(p.strip() for p in all_pages if p)\n",
    "            joined = re.sub(r\"\\s+\", \" \", joined)\n",
    "            print('Character Length = ', len(joined))\n",
    "            print('\\n DISPLAY SOME TEXT: \\n', joined[:100])\n",
    "            print('\\n Cleaner Text: ', joined[:200])\n",
    "\n",
    "            self.cleaner_Data = joined\n",
    "\n",
    "        else:\n",
    "            # ---- HTML path (mimics the class pattern) ----\n",
    "            html_xters = urlopen(url).read()\n",
    "            html_Data = html_xters.decode(\"utf-8\", \"ignore\")\n",
    "            print('Character Length = ', len(html_Data))\n",
    "            print('\\n DISPLAY SOME TEXT: \\n', html_Data[:100])\n",
    "\n",
    "            cleaner_Data = BeautifulSoup(html_Data, features=\"lxml\").get_text()\n",
    "            cleaner_Data = ' '.join(cleaner_Data.split())\n",
    "\n",
    "            print('\\n Cleaner Text: ', cleaner_Data[:200])\n",
    "            self.cleaner_Data = cleaner_Data\n",
    "\n",
    "    def Sentence_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Tokenize into sentences, lowercase, and remove stray newlines.\n",
    "        (Follows the user's method naming and printing style.)\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(self.cleaner_Data) if self._use_nltk else _simple_sent_tokenize(self.cleaner_Data)\n",
    "        sentences = [w.replace('\\n', '').lower() for w in sentences]\n",
    "        print('\\n Number of Sentences = ', len(sentences))\n",
    "        print('\\n Some Sentences: \\n', sentences[:10])\n",
    "        self._sentences = sentences\n",
    "\n",
    "    def Word_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Tokenize into words and print some samples.\n",
    "        \"\"\"\n",
    "        all_words = word_tokenize(self.cleaner_Data) if self._use_nltk else _simple_word_tokenize(self.cleaner_Data)\n",
    "        print('\\n Number of Words = ', len(all_words))\n",
    "        print('\\n Some Words: \\n', all_words[:10])\n",
    "        self.all_words = all_words\n",
    "\n",
    "    def getRid_meaningless_xters(self):\n",
    "        \"\"\"\n",
    "        Remove punctuation and English stopwords (keeps alphabetic tokens only).\n",
    "        \"\"\"\n",
    "        words_minus_punct = []\n",
    "        for wd in self.all_words:\n",
    "            if wd.isalpha():\n",
    "                words_minus_punct.append(wd.lower())\n",
    "\n",
    "        words_minus_conjunctions = []\n",
    "        conjunctions = self._stopwords\n",
    "\n",
    "        for wd in words_minus_punct:\n",
    "            if wd not in conjunctions:\n",
    "                words_minus_conjunctions.append(wd)\n",
    "\n",
    "        print('\\n', words_minus_conjunctions[:20])\n",
    "        self.meaningful_words = words_minus_conjunctions\n",
    "\n",
    "    def Word_Distr_visualizer(self):\n",
    "        \"\"\"\n",
    "        Plot top 20 words using NLTK's FreqDist (same style as user's code).\n",
    "        \"\"\"\n",
    "        word_freq = FreqDist(self.meaningful_words)\n",
    "        print('\\n', word_freq.most_common(20))\n",
    "        word_freq.plot(20)\n",
    "\n",
    "    def decode_message(self):\n",
    "        \"\"\"\n",
    "        Create a word cloud from the meaningful words.\n",
    "        Mirrors the user's method name; uses a 10s interval throttle by default.\n",
    "        \"\"\"\n",
    "        now = time.time()\n",
    "        if now - self._last_cloud_time < CLOUD_INTERVAL:\n",
    "            # respect interval; silently skip if too soon\n",
    "            return\n",
    "        self._last_cloud_time = now\n",
    "\n",
    "        if not self.meaningful_words:\n",
    "            return\n",
    "\n",
    "        decision_keywords = WordCloud().generate(\" \".join(self.meaningful_words))\n",
    "        vis.figure(figsize=(14, 14))\n",
    "        vis.axis(\"off\")\n",
    "        vis.imshow(decision_keywords)\n",
    "\n",
    "        # ---------------------------- Drive the class ----------------------------\n",
    "\n",
    "handle = EBOOK_web_scraper()\n",
    "\n",
    "print('\\n SCRAPING EBOOK')\n",
    "handle.accessData(ebook_url)\n",
    "\n",
    "# Tokenize Sentences\n",
    "print('\\n UNCLEAN TOKENIZED SENTENCES')\n",
    "handle.Sentence_tokenizer()\n",
    "\n",
    "# Tokenize Words\n",
    "print('\\n UNCLEAN TOKENIZED WORDS')\n",
    "handle.Word_tokenizer()\n",
    "\n",
    "# Get Rid of Meaningless Words and Visualize\n",
    "print('\\n MEANINGFUL WORDS')\n",
    "handle.getRid_meaningless_xters()\n",
    "\n",
    "# Frequency of Words and Visualization\n",
    "if RUN_FREQ_VIS:\n",
    "    print('\\n FREQUENCY: MOST COMMON WORDS AND VISUALIZATION')\n",
    "    handle.Word_Distr_visualizer()\n",
    "\n",
    "# Visualize and Decode Messages (WordCloud)\n",
    "if MAKE_CLOUDS:\n",
    "    print('\\n VISUAL DECODING FOR QUALITATIVE SENTIMENT')\n",
    "    handle.decode_message()\n",
    "\n",
    "# Clean up any temp PDF if created\n",
    "try:\n",
    "    if getattr(handle, \"_tmp_pdf_path\", None):\n",
    "        Path(handle._tmp_pdf_path).unlink(missing_ok=True)\n",
    "except Exception:\n",
    "    pass"
   ],
   "id": "496770899ac4ba2e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
